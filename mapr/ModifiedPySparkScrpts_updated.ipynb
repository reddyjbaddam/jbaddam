{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession object at 0x7f12c5d3e450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        databaseName|\n",
      "+--------------------+\n",
      "|             backend|\n",
      "|            backend1|\n",
      "|             default|\n",
      "|       demodb00_hcat|\n",
      "|       demodb01_hcat|\n",
      "|       demodb03_hcat|\n",
      "|         demodb_hcat|\n",
      "|                  go|\n",
      "|              movies|\n",
      "|            navntest|\n",
      "|           navntest1|\n",
      "|           perf_tpch|\n",
      "|            sameeeee|\n",
      "|              sample|\n",
      "|         sample_navn|\n",
      "|sample_navn_witho...|\n",
      "|                test|\n",
      "|              test01|\n",
      "|               test2|\n",
      "|         test_navn01|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 1462766 1461800 1462276 1461756 1460586 F 7309184\n",
      "O 1464013 1461240 1459703 1461850 1460658 O 7307464\n",
      "P 76314 77021 76961 76654 76402 P 383352\n",
      "*************Total count ************************************\n",
      "3\n",
      "****** Printing Total  Dataset ******************************\n",
      "3003093 3000061 2998940 3000260 2997646\n",
      "15000000\n",
      "****** Printing Time taken ******************************\n",
      "29.819247961\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"o_orderstatus\"\n",
    "# measure-> 1 count from orders\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from orders(TPCH) as bdaordersperf \n",
    "t1 = time()\n",
    "\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from tpch10.orders\";\n",
    "ds = spark.sql(sqlString)\n",
    "\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "\n",
    "dforders = spark.table(\"tpch10.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"tpch10_orders_\" + column)\n",
    "rows = dforders.groupBy(\"tpch10_orders_o_orderstatus\").pivot(\"tpch10_orders_o_orderpriority\").agg(func.count('tpch10_orders_ROW_NUM')).sort(\"tpch10_orders_o_orderstatus\")\n",
    "\n",
    "rows1 = dforders.groupBy(\"tpch10_orders_o_orderstatus\").agg(func.count('tpch10_orders_ROW_NUM')).sort(\"tpch10_orders_o_orderstatus\") \n",
    "#rows1.show();\n",
    "\n",
    "rows = rows.join(rows1,rows.tpch10_orders_o_orderstatus == rows1.tpch10_orders_o_orderstatus,\"inner\")\n",
    "\n",
    "rows = rows.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "\n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "totals = dforders.groupBy().pivot(\"tpch10_orders_o_orderpriority\").agg(func.count('tpch10_orders_ROW_NUM'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.count('tpch10_orders_ROW_NUM'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "print('****** Printing Time taken ******************************')\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 150217.953676 150360.2419 150107.554346 150142.456041 150265.569962 F 150218.740521\n",
      "O 150218.541297 150173.386023 150191.375195 150240.910075 150093.931562 O 150183.652375\n",
      "P 184776.741132 185048.370787 184401.316909 184914.398212 185078.166727 P 184843.545656\n",
      "*************Total count ************************************\n",
      "3\n",
      "****** Printing Total  Dataset ******************************\n",
      "151096.441152 151159.783341 151028.424989 151078.819384 151069.216228\n",
      "151086.54605\n",
      "****** Printing Time taken ******************************\n",
      "20.6066679955\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"o_orderstatus\"\n",
    "# measure-> 1 mean('o_totalprice') from orders\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from orders(TPCH) as orders \n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from tpch10.orders\";\n",
    "ds = spark.sql(sqlString)\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "\n",
    "\n",
    "dforders = spark.table(\"tpch10.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"tpch10_orders_\" + column)\n",
    "rows = dforders.groupBy(\"tpch10_orders_o_orderstatus\")\\\n",
    "             .pivot(\"tpch10_orders_o_orderpriority\").agg(func.mean('tpch10_orders_o_totalprice'))\\\n",
    "             .sort(\"tpch10_orders_o_orderstatus\")\n",
    "\n",
    "#rows.show();\n",
    "rows1 = dforders.groupBy(\"tpch10_orders_o_orderstatus\")\\\n",
    "             .agg(func.mean('tpch10_orders_o_totalprice'))\\\n",
    "             .sort(\"tpch10_orders_o_orderstatus\") \n",
    "#rows1.show();\n",
    "rows = rows.join(rows1,rows.tpch10_orders_o_orderstatus == rows1.tpch10_orders_o_orderstatus,\"inner\")\n",
    "#rows.show();\n",
    "rows = rows.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "    \n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "totals = dforders.groupBy().pivot(\"tpch10_orders_o_orderpriority\").agg(func.mean('tpch10_orders_o_totalprice'))\n",
    "\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "firstRowTotals = totals.first()\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.mean('tpch10_orders_o_totalprice'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "print('****** Printing Time taken ******************************')\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Magellan - PySpark",
   "language": "python",
   "name": "magellan_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.15\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
