{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        databaseName|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|         default_new|\n",
      "|        defaultgsdfg|\n",
      "|          infofusion|\n",
      "| infofusion_mapr_orc|\n",
      "|infofusion_mapr_p...|\n",
      "|              thcp10|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "println(spark.sessionState.conf.bucketingEnabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use thcp10 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|  thcp10|bucketed_10_c_cus...|      false|\n",
      "|  thcp10|bucketed_10_l_ord...|      false|\n",
      "|  thcp10|bucketed_10_o_ord...|      false|\n",
      "|  thcp10|bucketed_10_witho...|      false|\n",
      "|  thcp10|bucketed_10_witho...|      false|\n",
      "|  thcp10|bucketed_16_witho...|      false|\n",
      "|  thcp10|    custkey_customer|      false|\n",
      "|  thcp10|            customer|      false|\n",
      "|  thcp10|            lineitem|      false|\n",
      "|  thcp10|   orderkey_lineitem|      false|\n",
      "|  thcp10|     orderkey_orders|      false|\n",
      "|  thcp10|              orders|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_ds = [ROW_NUM: bigint, c_custkey: int ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ROW_NUM: bigint, c_custkey: int ... 7 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customer_ds = spark.sql(\"select *from customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineitem_ds = [ROW_NUM: bigint, l_orderkey: int ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ROW_NUM: bigint, l_orderkey: int ... 15 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lineitem_ds=spark.sql(\"select *from lineitem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_ds = [ROW_NUM: bigint, o_orderkey: int ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ROW_NUM: bigint, o_orderkey: int ... 8 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_ds=spark.sql(\"select *from orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//With 10 buckets\n",
    "val t1 = System.nanoTime\n",
    "customer_ds.write.bucketBy(10, \"c_custkey\").sortBy(\"c_custkey\").mode(SaveMode.Overwrite).saveAsTable(\"bucketed_10_c_custkey_customer\")\n",
    "lineitem_ds.write.bucketBy(10, \"l_suppkey\",\"l_orderkey\").sortBy(\"l_orderkey\").mode(SaveMode.Overwrite).saveAsTable(\"bucketed_10_l_orderkey_lineitem\")\n",
    "orders_ds.write.bucketBy(10, \"o_orderkey\").sortBy(\"o_orderkey\").mode(SaveMode.Overwrite).saveAsTable(\"bucketed_10_o_orderkey_orders\")\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customer_ds.write.mode(SaveMode.Overwrite).saveAsTable(\"custkey_customer\")\n",
    "lineitem_ds.write.mode(SaveMode.Overwrite).saveAsTable(\"orderkey_lineitem\")\n",
    "orders_ds.write.mode(SaveMode.Overwrite).saveAsTable(\"orderkey_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.408084595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1 = 2941192747651739\n",
       "custds = [ROW_NUM: bigint, c_custkey: int ... 7 more fields]\n",
       "lineitemds = [ROW_NUM: bigint, l_orderkey: int ... 15 more fields]\n",
       "orderds = [ROW_NUM: bigint, o_orderkey: int ... 8 more fields]\n",
       "duration = 52.408084595\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52.408084595"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//With total columns in the Tables with out bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select* from custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select *from orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select * from orderkey_orders\")\n",
    "spark.sql(\"select *from orderkey_lineitem ol ,orderkey_orders oo,custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//With selected columns in the Tables with out bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select c_custkey from custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select l_suppkey,l_orderkey from orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select o_orderkey from orderkey_orders\")\n",
    "spark.sql(\"select *from orderkey_lineitem ol ,orderkey_orders oo,custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "//With total columns in the Tables with bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select *from bucketed_10_c_custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select *from bucketed_10_l_orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select *from bucketed_10_o_orderkey_orders\")\n",
    "spark.sql(\"select *from bucketed_10_l_orderkey_lineitem ol ,bucketed_10_o_orderkey_orders oo,bucketed_10_c_custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//With selected columns in the Tables with bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select c_custkey from bucketed_10_c_custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select l_suppkey,l_orderkey from bucketed_10_l_orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select o_orderkey from bucketed_10_o_orderkey_orders\")\n",
    "spark.sql(\"select ol.l_orderkey from bucketed_10_l_orderkey_lineitem ol ,bucketed_10_o_orderkey_orders oo,bucketed_10_c_custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//With out sort and line item with 16 buckets\n",
    "val t1 = System.nanoTime\n",
    "customer_ds.write.bucketBy(3, \"c_custkey\").saveAsTable(\"bucketed_10_withoutsort_c_custkey_customer\")\n",
    "lineitem_ds.write.bucketBy(70, \"l_suppkey\",\"l_orderkey\").saveAsTable(\"bucketed_16_withoutsort_l_orderkey_lineitem\")\n",
    "orders_ds.write.bucketBy(8, \"o_orderkey\").saveAsTable(\"bucketed_10_withoutsort_o_orderkey_orders\")\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//With selected columns in the Tables with bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select *from bucketed_10_withoutsort_c_custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select *from bucketed_16_withoutsort_l_orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select *from bucketed_10_withoutsort_o_orderkey_orders\")\n",
    "spark.sql(\"select *from bucketed_16_withoutsort_l_orderkey_lineitem ol ,bucketed_10_withoutsort_o_orderkey_orders oo,bucketed_10_withoutsort_c_custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Below Modified by Abhishek and the above is remain intact creatd by jalendhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// crosstab specifications\n",
    "// dimension->1 (from the different table)\n",
    "// measure-> 1 count from customer \n",
    "// dimension from household\n",
    "\n",
    "t1 = time()\n",
    "dfHousehold = spark.table(\"default.householdengland\")\n",
    "for column in dfHousehold.columns: \n",
    "   dfHousehold = dfHousehold.withColumnRenamed(column, \"default_householdengland_\" + column);\n",
    "#dfHousehold.printSchema();\n",
    "dfCustomer = spark.table(\"default.customerengland\")\n",
    "for column in dfCustomer.columns:\n",
    "    dfCustomer = dfCustomer.withColumnRenamed(column, \"default_customerengland_\" + column);\n",
    "#dfCustomer.printSchema();\n",
    "dfJoin1 = dfHousehold.join(dfCustomer,dfHousehold.default_householdengland_HouseholdID == dfCustomer.default_customerengland_householdid,\"inner\")\n",
    "dfJoin1 = dfJoin1.dropDuplicates(['default_householdengland_propertytypedecoded','default_customerengland_custid'])\n",
    "rows = dfJoin1.groupBy(\"default_householdengland_propertytypedecoded\").agg(func.count(func.lit(1)).alias(\"Customers\"))\\\n",
    "             .sort(\"default_householdengland_propertytypedecoded\")\n",
    "print rows.count()\n",
    "rows = rows.limit(1000);\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1]\n",
    "totals = dfJoin1.agg(func.count(func.lit(1)).alias(\"Customers\"))\n",
    "firstRow =  totals.first()\n",
    "print firstRow[0]\n",
    "     \n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.30013792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1 = 2941353188575042\n",
       "duration = 67.30013792\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "67.30013792"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//With 10 buckets using the above existing one and just modified the order table to sort it as as per o_custkey and sort by o_custkey\n",
    "\n",
    "val t1 = System.nanoTime\n",
    "// customer_ds.write.bucketBy(10, \"c_custkey\").sortBy(\"c_custkey\").mode(SaveMode.Overwrite).saveAsTable(\"bucketed_10_c_custkey_customer\")\n",
    "// lineitem_ds.write.bucketBy(10, \"l_suppkey\",\"l_orderkey\").sortBy(\"l_orderkey\").mode(SaveMode.Overwrite).saveAsTable(\"bucketed_10_l_orderkey_lineitem\")\n",
    "orders_ds.write.bucketBy(10, \"o_custkey\").sortBy(\"o_custkey\").mode(SaveMode.Overwrite).saveAsTable(\"b_10_o_orderkey_orders\")\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "//The below step is just to create the table from takeing the original(loaded) data. \n",
    "customer_ds.write.mode(SaveMode.Overwrite).saveAsTable(\"custkey_customer\")\n",
    "lineitem_ds.write.mode(SaveMode.Overwrite).saveAsTable(\"orderkey_lineitem\")\n",
    "orders_ds.write.mode(SaveMode.Overwrite).saveAsTable(\"orderkey_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.117035235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1 = 2941540833043172\n",
       "custds = [ROW_NUM: bigint, c_custkey: int ... 7 more fields]\n",
       "lineitemds = [ROW_NUM: bigint, l_orderkey: int ... 15 more fields]\n",
       "orderds = [ROW_NUM: bigint, o_orderkey: int ... 8 more fields]\n",
       "duration = 51.117035235\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "51.117035235"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//With total columns in the Tables with out bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select* from custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select *from orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select * from orderkey_orders\")\n",
    "spark.sql(\"select *from orderkey_lineitem ol ,b_orderkey_orders oo,custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.480577422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1 = 2941620631585702\n",
       "custds = [ROW_NUM: bigint, c_custkey: int ... 7 more fields]\n",
       "lineitemds = [ROW_NUM: bigint, l_orderkey: int ... 15 more fields]\n",
       "orderds = [ROW_NUM: bigint, o_orderkey: int ... 8 more fields]\n",
       "duration = 52.480577422\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52.480577422"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//With total columns in the Tables with out bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select* from customer\")\n",
    "val lineitemds=spark.sql(\"select *from lineitem\")\n",
    "val orderds=spark.sql(\"select * from orders\")\n",
    "spark.sql(\"select *from lineitem ol ,orders oo,customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.818050014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1 = 2941874693377824\n",
       "custds = [ROW_NUM: bigint, c_custkey: int ... 7 more fields]\n",
       "lineitemds = [ROW_NUM: bigint, l_orderkey: int ... 15 more fields]\n",
       "orderds = [ROW_NUM: bigint, o_orderkey: int ... 8 more fields]\n",
       "duration = 61.818050014\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "61.818050014"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//With total columns in the Tables with bucketing\n",
    "val t1 = System.nanoTime\n",
    "val custds=spark.sql(\"select *from bucketed_10_c_custkey_customer\")\n",
    "val lineitemds=spark.sql(\"select *from bucketed_10_l_orderkey_lineitem\")\n",
    "val orderds=spark.sql(\"select *from b_10_o_orderkey_orders\")\n",
    "spark.sql(\"select *from bucketed_10_l_orderkey_lineitem ol ,b_10_o_orderkey_orders oo,bucketed_10_c_custkey_customer cc where ol.l_orderkey=oo.o_orderkey and cc.c_custkey=ol.l_suppkey\").count()\n",
    "val duration = (System.nanoTime - t1) / 1e9d\n",
    "println(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Magellan - Scala",
   "language": "scala",
   "name": "magellan_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
