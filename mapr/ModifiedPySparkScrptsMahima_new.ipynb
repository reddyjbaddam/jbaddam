{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://inhyd-bdamapr-client.lab.opentext.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2.0-mapr-1901</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mapr_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe16c5ad150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       databaseName|\n",
      "+-------------------+\n",
      "|            default|\n",
      "|        default_new|\n",
      "|       defaultgsdfg|\n",
      "|               demo|\n",
      "|         infofusion|\n",
      "|infofusion_mapr_orc|\n",
      "|             sample|\n",
      "|             thcp10|\n",
      "|             tpch10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.driver.maxResultSize\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlString = \"select o_orderpriority from sample.orders\";\n",
    "ds = spark.sql(sqlString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|o_orderpriority|\n",
      "+---------------+\n",
      "|          5-LOW|\n",
      "|       1-URGENT|\n",
      "|          5-LOW|\n",
      "|          5-LOW|\n",
      "+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 1462766 1461800 1462276 1461756 1460586 F 7309184\n",
      "O 1464013 1461240 1459703 1461850 1460658 O 7307464\n",
      "P 76314 77021 76961 76654 76402 P 383352\n",
      "*************Total count ************************************\n",
      "3\n",
      "****** Printing Total  Dataset ******************************\n",
      "3003093 3000061 2998940 3000260 2997646\n",
      "15000000\n",
      "****** Printing Time taken ******************************\n",
      "30.2608239651\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"o_orderstatus\"\n",
    "# measure-> 1 count from orders\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from orders(TPCH) as bdaordersperf \n",
    "t1 = time()\n",
    "\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from sample.orders\";\n",
    "ds = spark.sql(sqlString)\n",
    "\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "\n",
    "dforders = spark.table(\"sample.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\").pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM')).sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "rows1 = dforders.groupBy(\"perf_orders_o_orderstatus\").agg(func.count('perf_orders_ROW_NUM')).sort(\"perf_orders_o_orderstatus\") \n",
    "#rows1.show();\n",
    "\n",
    "rows = rows.join(rows1,rows.perf_orders_o_orderstatus == rows1.perf_orders_o_orderstatus,\"inner\")\n",
    "\n",
    "rows = rows.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "\n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "print('****** Printing Time taken ******************************')\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 150217.953676 150360.2419 150107.554346 150142.456041 150265.569962 F 150218.740521\n",
      "O 150218.541297 150173.386023 150191.375195 150240.910075 150093.931562 O 150183.652375\n",
      "P 184776.741132 185048.370787 184401.316909 184914.398212 185078.166727 P 184843.545656\n",
      "*************Total count ************************************\n",
      "3\n",
      "****** Printing Total  Dataset ******************************\n",
      "151096.441152 151159.783341 151028.424989 151078.819384 151069.216228\n",
      "151086.54605\n",
      "19.4574680328\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"o_orderstatus\"\n",
    "# measure-> 1 mean('o_totalprice') from orders\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from orders(TPCH) as bdaordersperf \n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from sample.bdaordersperf\";\n",
    "ds = spark.sql(sqlString)\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "\n",
    "\n",
    "dforders = spark.table(\"sample.bdaordersperf\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.mean('perf_orders_o_totalprice'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "#rows.show();\n",
    "rows1 = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.mean('perf_orders_o_totalprice'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "#rows1.show();\n",
    "rows = rows.join(rows1,rows.perf_orders_o_orderstatus == rows1.perf_orders_o_orderstatus,\"inner\")\n",
    "#rows.show();\n",
    "rows = rows.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "    \n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.mean('perf_orders_o_totalprice'))\n",
    "\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "firstRowTotals = totals.first()\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.mean('perf_orders_o_totalprice'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "****** Printing Time taken ******************************\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Total  Dataset ******************************\n",
      "3003093 3000061 2998940 3000260 2997646\n",
      "15000000\n",
      "****** Printing Total  Dataset for second measure ******************************\n",
      "600577.46 600095.81 599572.4 599881.83 599245.74\n",
      "2999373.24\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 1462766 1461800 1462276 1461756 1460586 7309184 290778.13 290766.35 290567.15 290456.62 290399.31 F\n",
      "O 1464013 1461240 1459703 1461850 1460658 7307464 291158.15 290450.17 290219.85 290624.73 290167.16 O\n",
      "P 76314 77021 76961 76654 76402 383352 18641.18 18879.29 18785.4 18800.48 18679.27 P\n",
      "*************Total count ************************************\n",
      "3\n",
      "558.369824886\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"PropertyType\"\n",
    "# measure->2 count from orders, sum(l_discount)--> Lineitem table\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from o_orderstatus\n",
    "t1 = time()\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from sample.orders\";\n",
    "ds = spark.sql(sqlString)\n",
    "\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "#################################\n",
    "#   ACTUAL and TOTAL RESULTS (1)\n",
    "################################\n",
    "\n",
    "dforders = spark.table(\"sample.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "\n",
    "rows1 = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.count('perf_orders_ROW_NUM'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "\n",
    "rows = rows.join(rows1,'perf_orders_o_orderstatus',\"inner\")\n",
    "#rows.show();\n",
    "\n",
    "######################\n",
    "# TOTAL (1)\n",
    "###############################\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "\n",
    "#################################\n",
    "#   ACTUAL and TOTAL RESULTS (2)\n",
    "################################\n",
    "dflineitem = spark.table(\"sample.lineitem\")\n",
    "for column in dflineitem.columns:\n",
    "    dflineitem = dflineitem.withColumnRenamed(column, \"perf_lineitem_\" + column);\n",
    "#dflineitem.printSchema();\n",
    "dfJoin1 = dforders.join(dflineitem,dforders.perf_orders_o_orderkey == dflineitem.perf_lineitem_l_orderkey,\"left\")\n",
    "dfJoin1 = dfJoin1.dropDuplicates(['perf_orders_o_orderstatus','perf_orders_o_orderpriority','perf_lineitem_ROW_NUM'])\n",
    "rows21 = dfJoin1.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.sum('perf_lineitem_l_discount'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "dfJoin2 = dfJoin1.dropDuplicates(['perf_orders_o_orderstatus','perf_lineitem_ROW_NUM'])\n",
    "rows22 = dfJoin2.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.sum('perf_lineitem_l_discount'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "        \n",
    "actualdsrows2 = rows21.join(rows22,rows21.perf_orders_o_orderstatus == rows22.perf_orders_o_orderstatus,\"inner\")\n",
    "#actualdsrows2.show()\n",
    "\n",
    "######################\n",
    "# TOTAL (2)\n",
    "###############################\n",
    "totals = dfJoin1.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.sum('perf_lineitem_l_discount'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset for second measure ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dfJoin2.groupBy().agg(func.sum('perf_lineitem_l_discount'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "\n",
    "###########################################\n",
    "#   COMBINING ACTUAL DS of TWO MESAURES\n",
    "#############################################\n",
    "rows.cache()\n",
    "actualdsrows2.cache()\n",
    "actualds = rows.join(actualdsrows2,'perf_orders_o_orderstatus',\"inner\")\n",
    "\n",
    "actualds = actualds.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in actualds.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7],row[8], row[9],  row[10], row[11], row[12]\n",
    "\n",
    "    \n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Total  Dataset ******************************\n",
      "3003093 3000061 2998940 3000260 2997646\n",
      "15000000\n",
      "****** Printing Total  Dataset for second measure ******************************\n",
      "600577.46 600095.81 599572.4 599881.83 599245.74\n",
      "2999373.24\n",
      "****** Printing Total  Dataset for measure 3 ******************************\n",
      "151096.441152 151159.783341 151028.424989 151078.819384 151069.216228\n",
      "151086.54605\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 1462766 1461800 1462276 1461756 1460586 7309184 290778.13 290766.35 290567.15 290456.62 290399.31 F 1452967.56 150217.953676 150360.2419 150107.554346 150142.456041 150265.569962 150218.740521\n",
      "O 1464013 1461240 1459703 1461850 1460658 7307464 291158.15 290450.17 290219.85 290624.73 290167.16 O 1452620.06 150218.541297 150173.386023 150191.375195 150240.910075 150093.931562 150183.652375\n",
      "P 76314 77021 76961 76654 76402 383352 18641.18 18879.29 18785.4 18800.48 18679.27 P 93785.62 184776.741132 185048.370787 184401.316909 184914.398212 185078.166727 184843.545656\n",
      "*************Total count ************************************\n",
      "3\n",
      "705.368054867\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications [3 measures]\n",
    "# dimension->1 (from the same table) \"PropertyType\"\n",
    "# measure->2 count from orders, sum(l_discount), mean(o_totalprice)-->order, Lineitem table\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from o_orderstatus\n",
    "t1 = time()\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from sample.orders\";\n",
    "ds = spark.sql(sqlString)\n",
    "\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "#################################\n",
    "#   ACTUAL and TOTAL RESULTS (1)\n",
    "################################\n",
    "\n",
    "dforders = spark.table(\"sample.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "\n",
    "rows1 = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.count('perf_orders_ROW_NUM'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "\n",
    "rows = rows.join(rows1,'perf_orders_o_orderstatus',\"inner\")\n",
    "#rows.show();\n",
    "\n",
    "######################\n",
    "# TOTAL (1)\n",
    "###############################\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "\n",
    "#################################\n",
    "#   ACTUAL and TOTAL RESULTS (2)\n",
    "################################\n",
    "dflineitem = spark.table(\"sample.lineitem\")\n",
    "for column in dflineitem.columns:\n",
    "    dflineitem = dflineitem.withColumnRenamed(column, \"perf_lineitem_\" + column);\n",
    "#dflineitem.printSchema();\n",
    "dfJoin1 = dforders.join(dflineitem,dforders.perf_orders_o_orderkey == dflineitem.perf_lineitem_l_orderkey,\"left\")\n",
    "dfJoin1 = dfJoin1.dropDuplicates(['perf_orders_o_orderstatus','perf_orders_o_orderpriority','perf_lineitem_ROW_NUM'])\n",
    "rows21 = dfJoin1.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.sum('perf_lineitem_l_discount'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "dfJoin2 = dfJoin1.dropDuplicates(['perf_orders_o_orderstatus','perf_lineitem_ROW_NUM'])\n",
    "rows22 = dfJoin2.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.sum('perf_lineitem_l_discount'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "        \n",
    "actualdsrows2 = rows21.join(rows22,rows21.perf_orders_o_orderstatus == rows22.perf_orders_o_orderstatus,\"inner\")\n",
    "#actualdsrows2.show()\n",
    "\n",
    "######################\n",
    "# TOTAL (2)\n",
    "###############################\n",
    "totals = dfJoin1.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.sum('perf_lineitem_l_discount'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset for second measure ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dfJoin2.groupBy().agg(func.sum('perf_lineitem_l_discount'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "#################################\n",
    "#   ACTUAL and TOTAL RESULTS (3)\n",
    "################################\n",
    "\n",
    "dforders = spark.table(\"sample.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rowsm3 = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.mean('perf_orders_o_totalprice'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "\n",
    "rows1m3 = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.mean('perf_orders_o_totalprice'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "\n",
    "rowsm3 = rowsm3.join(rows1m3,'perf_orders_o_orderstatus',\"inner\")\n",
    "#rows.show();\n",
    "\n",
    "######################\n",
    "# TOTAL (3)\n",
    "###############################\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.mean('perf_orders_o_totalprice'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset for measure 3 ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.mean('perf_orders_o_totalprice'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "\n",
    "###########################################\n",
    "#   COMBINING ACTUAL DS of TWO MESAURES\n",
    "#############################################\n",
    "rows.cache()\n",
    "actualdsrows2.cache()\n",
    "actualds = rows.join(actualdsrows2,'perf_orders_o_orderstatus',\"inner\")\n",
    "\n",
    "actualds.cache()\n",
    "rowsm3.cache()\n",
    "actualds = actualds.join(rowsm3,'perf_orders_o_orderstatus',\"inner\")\n",
    "\n",
    "actualds = actualds.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in actualds.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7],row[8], row[9],  row[10], row[11], row[12], row[13], row[14],  row[15], row[16], row[17], row[18], row[19]\n",
    "\n",
    "    \n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dforders = spark.table(\"sample.orders\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+-------+--------+---------------+-------+\n",
      "|perf_orders_o_orderstatus|1-URGENT| 2-HIGH|3-MEDIUM|4-NOT SPECIFIED|  5-LOW|\n",
      "+-------------------------+--------+-------+--------+---------------+-------+\n",
      "|                        F| 1462766|1461800| 1462276|        1461756|1460586|\n",
      "|                        O| 1464013|1461240| 1459703|        1461850|1460658|\n",
      "|                        P|   76314|  77021|   76961|          76654|  76402|\n",
      "+-------------------------+--------+-------+--------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dflineitem = spark.table(\"sample.lineitem\")\n",
    "for column in dflineitem.columns:\n",
    "    dflineitem = dflineitem.withColumnRenamed(column, \"perf_lineitem_\" + column);\n",
    "#dflineitem.printSchema();\n",
    "dfJoin1 = dforders.join(dflineitem,dforders.perf_orders_o_orderkey == dflineitem.perf_lineitem_l_orderkey,\"left\")\n",
    "dfJoin1 = dfJoin1.dropDuplicates(['perf_orders_o_orderstatus','perf_orders_o_orderpriority','perf_lineitem_ROW_NUM'])\n",
    "rows21 = dfJoin1.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.sum('perf_lineitem_l_discount'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "|perf_orders_o_orderstatus|         1-URGENT|           2-HIGH|          3-MEDIUM|   4-NOT SPECIFIED|             5-LOW|\n",
      "+-------------------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "|                        F|290778.1299999097|290766.3499999097|   290567.14999991| 290456.6199999101|290399.30999990983|\n",
      "|                        O|291158.1499999092|  290450.16999991|290219.84999991016| 290624.7299999096| 290167.1599999102|\n",
      "|                        P|18641.18000000002|18879.29000000002|18785.400000000005|18800.480000000014|18679.270000000015|\n",
      "+-------------------------+-----------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows21.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+---------------+-------+\n",
      "|1-URGENT| 2-HIGH|3-MEDIUM|4-NOT SPECIFIED|  5-LOW|\n",
      "+--------+-------+--------+---------------+-------+\n",
      "| 3003093|3000061| 2998940|        3000260|2997646|\n",
      "+--------+-------+--------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999373.24\n"
     ]
    }
   ],
   "source": [
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
