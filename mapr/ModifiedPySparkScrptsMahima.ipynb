{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ussm-mag166-systest-hdp30-01.lab.opentext.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1.3.0.0.0-1634</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>otadmin_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4604541550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        databaseName|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|                demo|\n",
      "|         infofusion1|\n",
      "|      infofusion_orc|\n",
      "|infofusion_orc_spark|\n",
      "|  infofusion_parquet|\n",
      "|infofusion_parque...|\n",
      "|          nifi_spark|\n",
      "|              sample|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 1462766 1461800 1462276 1461756 1460586 F 7309184\n",
      "O 1464013 1461240 1459703 1461850 1460658 O 7307464\n",
      "P 76314 77021 76961 76654 76402 P 383352\n",
      "*************Total count ************************************\n",
      "3\n",
      "****** Printing Total  Dataset ******************************\n",
      "3003093 3000061 2998940 3000260 2997646\n",
      "15000000\n",
      "****** Printing Time taken ******************************\n",
      "18.645116806\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"o_orderstatus\"\n",
    "# measure-> 1 count from orders\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from orders(TPCH) as bdaordersperf \n",
    "t1 = time()\n",
    "\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from sample.bdaordersperf\";\n",
    "ds = spark.sql(sqlString)\n",
    "\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "\n",
    "dforders = spark.table(\"sample.bdaordersperf\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\").pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM')).sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "rows1 = dforders.groupBy(\"perf_orders_o_orderstatus\").agg(func.count('perf_orders_ROW_NUM')).sort(\"perf_orders_o_orderstatus\") \n",
    "#rows1.show();\n",
    "\n",
    "rows = rows.join(rows1,rows.perf_orders_o_orderstatus == rows1.perf_orders_o_orderstatus,\"inner\")\n",
    "\n",
    "rows = rows.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "\n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRowTotals = totals.first()\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.count('perf_orders_ROW_NUM'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "print('****** Printing Time taken ******************************')\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Printing Pivot Column Discrete values***************\n",
      "1-URGENT\n",
      "2-HIGH\n",
      "3-MEDIUM\n",
      "4-NOT SPECIFIED\n",
      "5-LOW\n",
      "****** Printing Actual Dataset ******************************\n",
      "F 150217.953676 150360.2419 150107.554346 150142.456041 150265.569962 F 150218.740521\n",
      "O 150218.541297 150173.386023 150191.375195 150240.910075 150093.931562 O 150183.652375\n",
      "P 184776.741132 185048.370787 184401.316909 184914.398212 185078.166727 P 184843.545656\n",
      "*************Total count ************************************\n",
      "3\n",
      "****** Printing Total  Dataset ******************************\n",
      "151096.441152 151159.783341 151028.424989 151078.819384 151069.216228\n",
      "151086.54605\n",
      "19.4574680328\n"
     ]
    }
   ],
   "source": [
    "# crosstab pivoted specifications\n",
    "# dimension->1 (from the same table) \"o_orderstatus\"\n",
    "# measure-> 1 mean('o_totalprice') from orders\n",
    "# pivot -> same table \"o_orderpriority\"\n",
    "# dimension from orders(TPCH) as bdaordersperf \n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print('**********Printing Pivot Column Discrete values***************')\n",
    "sqlString = \"select o_orderpriority from sample.bdaordersperf\";\n",
    "ds = spark.sql(sqlString)\n",
    "distinctds = ds.distinct().sort(\"o_orderpriority\")\n",
    "distinctds=distinctds.limit(1000)\n",
    "for value in distinctds.collect():\n",
    "   print value[0]\n",
    "\n",
    "\n",
    "dforders = spark.table(\"sample.bdaordersperf\")\n",
    "for column in dforders.columns:\n",
    "   dforders = dforders.withColumnRenamed(column,\"perf_orders_\" + column)\n",
    "rows = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .pivot(\"perf_orders_o_orderpriority\").agg(func.mean('perf_orders_o_totalprice'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\")\n",
    "\n",
    "#rows.show();\n",
    "rows1 = dforders.groupBy(\"perf_orders_o_orderstatus\")\\\n",
    "             .agg(func.mean('perf_orders_o_totalprice'))\\\n",
    "             .sort(\"perf_orders_o_orderstatus\") \n",
    "#rows1.show();\n",
    "rows = rows.join(rows1,rows.perf_orders_o_orderstatus == rows1.perf_orders_o_orderstatus,\"inner\")\n",
    "#rows.show();\n",
    "rows = rows.limit(1000);\n",
    "print('****** Printing Actual Dataset ******************************')\n",
    "for row in rows.collect():\n",
    "    print row[0], row[1],  row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "    \n",
    "print('*************Total count ************************************')\n",
    "print rows.count()\n",
    "\n",
    "totals = dforders.groupBy().pivot(\"perf_orders_o_orderpriority\").agg(func.mean('perf_orders_o_totalprice'))\n",
    "\n",
    "print('****** Printing Total  Dataset ******************************')\n",
    "firstRowTotals = totals.first()\n",
    "print firstRowTotals[0],firstRowTotals[1],firstRowTotals[2],firstRowTotals[3],firstRowTotals[4]\n",
    "\n",
    "totals1 = dforders.groupBy().agg(func.mean('perf_orders_o_totalprice'))\n",
    "firstRow1 = totals1.first()\n",
    "print firstRow1[0]\n",
    "\n",
    "****** Printing Time taken ******************************\n",
    "print time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Magellan - PySpark",
   "language": "python",
   "name": "magellan_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.15\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
